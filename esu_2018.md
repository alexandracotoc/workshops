![alt text](https://github.com/nolauren/workshops/blob/master/img/dh.jpg " ")




# Week 1

## Tuesday
| Time | Topic| Relevant Links |
| ------------- |-------------| -----|
| 11:15-12:45 | Introductions, Overview of the Course, Classroom Expectations   | |
| 12:45-2:15 |  Lunch  | |
| 2:15-3:45| [Determine the Data and Object of Study](https://docs.google.com/presentation/d/1g_VIGo5Jnfub2b_cm2PkYW_ZF0S4op_4vWy5Gkq6clU/edit) |  |
| 7:30 | Communal Dinner | |

Homework:

## Wednesday
| Time | Topic| Relevant Links |
| ------------- |-------------| -----|
| 9:15-10:45 | [What is Humanities Data?](https://docs.google.com/presentation/d/1MLZbGi1SazayWry9uMi3kYRzeUs4K8cpB3RGykZemHE/edit?usp=sharing); [Data Database Activity](https://docs.google.com/spreadsheets/d/1PYhBczLNUcG4PvZSHMhcXiYDOIcRUejMA41Y8kutaws/edit?usp=sharing) |  |
| 10:45-11:15 | Break | |
| 11:15-12:45 | [Metadata](https://docs.google.com/presentation/d/1SExIy1UKTn6YFNF5WpGhAS-crGmu-0ZIsGqDsOdPsms/edit?usp=sharing) ; Build Movie Data Set Activity |  |
| 12:45-2:15 | Lunch | |
| 2:15-3:45 | Project Sessions  |  |
| 3:45-4:15 | Break |  |
| 4:15-5:45 | Lecture |  |


## Thursday
| Time | Topic| Relevant Links |
| ------------- |-------------| -----|
| 9:15-10:45 | Teaser Session |  |
| 10:45-11:15 | Break | |
| 11:15-12:45 | Metadata, Brief Viz |  |
| 12:45-2:15  | Lunch |  |
| 2:15-3:45 | Tidy Data, Rebuild the Data Set  |  |
| 3:45-7:15 | Field Trip |  |
| 7:30 | Communal Dinner | |



## Friday
| Time | Topic| Relevant Links |
| ------------- |-------------| -----|
| 9:15-10:45 | Tidy Data | [ISO](https://www.iso.org/home.html), [LOC Genre](https://www.loc.gov/rr/mopic/miggen.html)  |
| 10:45-11:15 | Break | |
| 11:15-12:45 | [Data](https://docs.google.com/presentation/d/1g_VIGo5Jnfub2b_cm2PkYW_ZF0S4op_4vWy5Gkq6clU/edit) |  |
| 12:45-2:15  | Lunch |  |
| 2:15-3:45 | Projects |  |
| 3:45-7:15 | Field Trip |  |
| 8:00 | Communal Dinner | |

## Saturday
| Time | Topic| Relevant Links |
| ------------- |-------------| -----|
| 9:15-10:45 |  Open Refine/ Assessing Your Data |  |
| 10:45-11:15| Break | |
| 11:15-12:45 | Reflection / Slam Prep  | [LOC Subject Headings](https://id.loc.gov/authorities/subjects.html) |
| 12:45-2:15  | Lunch |  |
| 2:15-3:45 | Slam |  |
| 3:45-4:15 | Break |  |
| 3:45-5:45 | Field Trip |  |
| 8:00 | Communal Dinner | |

----------------------------

# Community Expectations
- Be kind, honest, and candid when expressing an opinion.
- Support building a collaborative environment including different styles of workshops such as working in groups.
- Courageous community in which experimenting is encouaged.  Be brave! 
- Signaling to the community by raising hands before talking. 
- Time at end of class for questions. 
- Time to experiment with tools. 
- A balance between theory and practice.
- Support interdisciplinarity.
- Draw on our own examples and experiences while respecting the feedback. 
- Make sure everyone is on the same page. 
- All questions are good question and encouraged! 
- I can ask my neighbor for help.
- Everyone has an opportunity to complete their sentences. 

# Week 2

## Monday
| Time | Topic| Relevant Links |
| ------------- |-------------| -----|
| 9:15-10:45 | Intro, Overview, [Text Analysis](https://docs.google.com/presentation/d/1ERw-cTgg1_UU4Z8-nVIyuYniQ4lMpxNryt_EWE0Zjgg/edit?usp=sharing)| 
| 10:45-11:15 | Break | | 
| 11:15-12:45 | Voyant  | |
| 12:45-2:15 |  Lunch  | |
| 2:15-3:45| Projects |  |
| 3:45-4:15 | Break |  |
| 4:15-5:45 | Lecture |  |

## Tuesday
| Time | Topic| Relevant Links |
| ------------- |-------------| -----|
| 9:15-10:45 | Mapping - Story Maps |  |
| 10:45-11:15 | Break | |
| 11:15-12:45 |  Carto |  |
| 12:45-2:15 | Lunch | |
| 2:15-3:45 | Project |  |
| 3:45-4:15 | Break |  |
| 4:15-5:45 | Lecture |  |
| 5:45-7:15 | Carto |  |

-------------------------- 
# Readings 

The following readings are optional. They are provided for future reference.

## Data

D'Ignazio, Catherine and Lauren F. Klein, ["Feminist Data Visualization."](https://drive.google.com/file/d/1xCQbOGl6KCuu7o1uQ2RplcqmfMPsqZ1E/view?usp=sharing). IEEE 

D'Ignazio, Catherine and Rahul Bhargava, *["DataBasic: design principles, tools and activities for Data Literacy Learners](http://ci-journal.org/index.php/ciej/article/view/1294/1229)*, The Journal of Community Informatics, 2016. 

Drucker, Johanna. "[Humanities Approaches to Graphical Display](http://www.digitalhumanities.org/dhq/vol/5/1/000091/000091.html)", 2011.

Drucker, Johanna. *[Graphesis
Visual Forms of Knowledge Production](http://www.hup.harvard.edu/catalog.php?isbn=9780674724938)*, 2014.

Gibbs, Frederick W. “[New Forms of History: Critiquing Data and Its Representations](http://tah.oah.org/february-2016/new-forms-of-history-critiquing-data-and-its-representations/).” The American Historian, 2016

Munoz, Trevor and Katie Rawson. "[Against Cleaning.](http://curatingmenus.org/articles/against-cleaning/", July 6, 2016.  

Munoz, Trevor. "[Data Curation as Publishing for the Digital Humanities](http://journalofdigitalhumanities.org/2-3/data-curation-as-publishing-for-the-digital-humanities/), 2013.

Posner, Miriam."[Humanities Data: A Necessary Contradiction
](http://miriamposner.com/blog/humanities-data-a-necessary-contradiction/)," 2015.

Robsenburg, Daniel. "[Data Before The Fact](http://static1.1.sqspcdn.com/static/f/1133095/23310656/1376447540493/Rosenburg_RawData.pdf)".

Palmer, Carole L, Nicholas M. Weber, Trevor Muñoz, Allen H. Renear. "[Foundations of Data Curation: The Pedagogy and Practice of 'Purposeful Work' with Research Data](http://www.archivejournal.net/essays/foundations-of-data-curation-the-pedagogy-and-practice-of-purposeful-work-with-research-data/)", 2013.

Padilla, Thomas. ["Humanities Data in the Library: Integrity, Form, Access".](http://www.dlib.org/dlib/march16/padilla/03padilla.html) D-Lib, March/April 2016. 

Schoch, Christof. "[Big? Smart? Clean? Messy? Data in the Humanities Journal of Digital Humanities](http://journalofdigitalhumanities.org/2-3/big-smart-clean-messy-data-in-the-humanities/),"
Journal of Digital Humanities, 2013.

Wickham, Hadley. "[Tidy Data](https://www.jstatsoft.org/article/view/v059i10)," 2014. Informal/code version [here](https://cran.r-project.org/web/packages/tidyr/vignettes/tidy-data.html).

Lincoln, Matthew. "[Best Practices for Using Google Sheets in your Data Project](https://matthewlincoln.net/2018/03/26/best-practices-for-using-google-sheets-in-your-data-project.html)"


## Text Analysis

Schultz, Kathryn. [“What is Distant Reading?” New York Times. June 26, 2011.](http://www.nytimes.com/2011/06/26/books/review/the-mechanic-muse-what-is-distant-reading.html)

Jockers, Matthew. *Macroanalysis*. Introduction.

*Graphs, Maps, Trees.* Verso. 2007.

Blevins, Cameron.[Topic Modeling Martha Ballard’s Diary](http://www.cameronblevins.org/martha-ballards-diary/)

Brett, Megan R. [“Topic Modeling: A Basic Introduction.” Journal of Digital Humanities. Vol 2. No. 1 Winter 2012.](http://journalofdigitalhumanities.org/2-1/topic-modeling-a-basic-introduction-by-megan-r-brett/)

Goldstone, Andrew and Ted Underwood. [“What Can Topic Models of PMLA Teach Us About the History of Literary Scholarship?” Journal of Digital Humanities. Vol 2. No. 1 Winter 2012.](http://journalofdigitalhumanities.org/2-1/what-can-topic-models-of-pmla-teach-us-by-ted-underwood-and-andrew-goldstone/)  

Meeks, Elijah and Scott Weingart. [“The Digital Humanities Contribution to Topic Modeling.” Journal of Digital Humanities. Vol 2. No. 1 Winter 2012.](http://journalofdigitalhumanities.org/2-1/dh-contribution-to-topic-modeling/)

Rhody, Lisa. [Topic Modeling and Figurative Language](http://journalofdigitalhumanities.org/2-1/topic-modeling-and-figurative-language-by-lisa-m-rhody/), 2012.

Rhody, Lisa. ["Why I Dig: Feminist Approaches to Text Analysis"](http://dhdebates.gc.cuny.edu/debates/text/97). Debates in the Digital Humanities, 2016.

Underwood, Ted. [Topic modeling made just simple enough.](https://tedunderwood.com/2012/04/07/topic-modeling-made-just-simple-enough/), 2012.

["Forum: Text Analysis at Scale."](http://dhdebates.gc.cuny.edu/debates/part/14). Debates in the Digital Humanities 2016.

Examples: [Mining the Dispatch](http://dsl.richmond.edu/dispatch/);[Signs@40](http://signsat40.signsjournal.org/)

## Networks

Easley, David. [Networks, Crowds, and Markets: Reasoning About a Highly Connected World](http://www.cambridge.org/gb/academic/subjects/computer-science/algorithmics-complexity-computer-algebra-and-computational-g/networks-crowds-and-markets-reasoning-about-highly-connected-world?format=HB&isbn=9780521195331#27IcB9j9LJIJgyYm.97), 2010.

Newman, Mark. *[Networks: An Introduction.](https://global.oup.com/academic/product/networks-9780199206650?cc=us&lang=en&)*, 2010.   

Weingart, Scott. [“Demystifying Networks, Part I & II.” Journal of Digital Humanities. Vol 1 No. 1. Winter 2011.](http://journalofdigitalhumanities.org/1-1/demystifying-networks-by-scott-weingart)

Example Projects: [Linked Jazz](https://linkedjazz.org/),[Republic of Letters](http://republicofletters.stanford.edu/), [Signs at 40](http://signsat40.signsjournal.org/cocitation/), and  [Wikipedia](xefer.com/WIKIPEDIA)


## Mapping

Bodenhamer, David, [“Beyond GIS: Geospatial Technologies and the Future of History.”](https://github.com/introdh2016/other/blob/master/BeyondGIS.pdf) History and GIS: Epistomologies, Considerations and Reflections. Springer, 2013.  

White, Richard. [“What is spatial history?” February 1, 2010](https://web.stanford.edu/group/spatialhistory/cgi-bin/site/pub.php?id=29)

Shnayder, Evgenia. [“A Data Model for Spatial History.” November
15, 2010.](https://web.stanford.edu/group/spatialhistory/cgi-bin/site/pub.php?id=23)

Crampton, Jeremy. [Mapping: A Critical Introduction to Cartography and GIS. Wiley-Blackwell, 2010.](https://github.com/introdh2016/other)

Examples: [American Panorama](http://dsl.richmond.edu/panorama/); [Anti-Eviction Mapping Project](https://www.antievictionmap.com/); [Photogrammar](photogrammar.yale.edu); Blevins, Cameron, "[Space, Nation, and the Triumph of Region: A View of the World from
Houston](http://cameronblevins.org/downloads/Blevins_SpaceNationAndTheTriumphOfRegion_Color.pdf)", *Journal of American History*, 2014.

------------

Ceeilia - @mcpmagalhaes

Alexandra - @alexandracotoc

Jeff - @jeffklo

Giuditta - @giudicirni

Elisabetta - @ec_giovannini

Carol - @digitaldante

Lauren - @nolauren @distantviewing

---------------


# Organizing Data



## Open Refine

OpenRefine (formerly known as GoogleRefine), is a very popular tool for working with unorganized, non-normalized (what some may call "messy") data. OpenRefine accepts TSV, CSV, XLS/XLSX, JSON, XML, RDF as XML, and Google Data formats, though others may be used with extensions. It works by opening into your default browser window, but all of the processing takes place on your machine and your data isn't uploaded anywhere. 


### Data

We will be using the movie data that we created in Google Sheets.  Download the "messymovies" data as a .csv file.

### Loading the Dataset
-  Click 'Open' in the top right to open a new OpenRefine tab
- Click 'Browse' and locate the .csv on your hard drive. Then click 'Next.'
- The Configure Parsing Options screen will ask you to confirm a few things. It has made guesses, based on the data, on the type of file, the character encoding and the character that separates columns. Take a look at the data in the top window and make sure everything looks like it's showing up correctly.
-  Name the project "movies-metadata" and click 'Create Project' in the top right corner.

#### Evaluation
Take a minute to look around. Consider the structure of the data with principles of "tidy data" in mind. This will help guide what types of operations you perform on the data. Also take time to evaluate the type of information that is represented and what type of questions you might want to ask of it.


## Movie Titles
Let's take a look at our movies. Does anything stand out?

### Multiple Spellings
- Select the column and then 'Facet', 'Text Facet'. A new window appears on the left. Let's explore. 
- Select 'count'. What do you notice? Why does this matter?
- Select 'cluster'. We have several options. Select all of the titles you want to merge and make sure to check off the 'Merge?' box for each. Next, select 'Merge Selected & Recluster' or 'Merge Selected & Close'.

### Redundant Films
 - Select a redundant film. Ex. Mad Max
 - Select 'Sort', 'Sort'.
 - Next to rows, there now appears 'Sort'.
 - Select the new 'Sort', 'Reorder Rows Permanently. 
 - Select 'movie_title', 'Edit cells', 'Blank Down'
 - Select 'move_title', 'Facet', 'Customized facets', 'Facets By Blank'
 - In the window on the left, select True 
 - Select "All" in our matching Rows, 'Edit Rows', 'Remove All Matching Rows'. 


### Country of Origin
- We want to split the countries into different columns. 
- Select the menu on the 'Column', 'Edit Column', 'Split Into Several Columns'.
- Since a comma is used to seperate our values, we will keep ','. 


### Date
We did the work to standardize our dates. Now, we need to tell OpenRefine that this is a date.
- Select 'Edit Cells', 'Common transforms', 'To date' 
- Now we see that it adds time. This is not ideal but fine. 
- We can then look at the timeline. Select 'Facet' and 'Timeline facet'

Let's take a look. What's going on here? Does this look ok?

### Budget

Let's take a look.
- Select 'Facet', 'Numeric Facet'

Wait, what's wrong? 

- 'Edit Cells', 'Common transforms', 'To Number'

Now let's facet it again. What do we learn?

- Let's edit the cell. Hover over the cell and select 'edit'.
- We also need to change the data type. Selec 'number'.



### Wins

Let's facet it and explore. Any issues?

- Select 'Facet', 'Text Facet'. 
- Select 'Y'
- Select 'Date_Released'
- Select 'Timeline Facet' and let's adjust our range.
- Let's go back and 'Edit Cells', 'Tranform'.

Here we see what is called GREL (General Refine Expression Language). We can use code to edit our data. This is very powerful! It opens up a plethora of ways to transform our data.
- In the expression box type, 'value.replace("Y", "N")'. What happened?

### Explore! 

--------------------------------------------------------

# Constructing Your Digital Identity with GitHub

##### Author: Taylor Arnold, Math & Computer Science, University of Richmond
------

GitHub is a web platform for archiving collections of code and
other materials into what are known as *repositories*. It is 
completely free to use for open source projects developed out
in the public. Paid accounts given access to private repositories
that can contain personal or closed-source materials. You are
currently looking at the repository named **user-template**. In
addition to providing tools for collaborative programming, GitHub
also provides a system for hosting static websites. These are known
collectively as *GitHub pages*. This tutorial will set-up a basic
GitHub pages that can serve as your digital identity. The tutorial
assumes that you already have signed up for a free GitHub account.
If not, go to [https://github.com/](https://github.com/) and create
and validate your username.

The end goal of the tutorial here is to create a website located 
at the following URL:

- https://[USERNAME].github.io/

With your username filled in for *[USERNAME]*. There are two typical
ways that this website can be used:

1. It can redirect to another page, such as a homepage at your
current place of study or employment, or
2. You can host a full-blown website on GitHub.

This tutorial will set-up the first option, by default redirecting to
your GitHub user page. To create a custom website, first follow the
instructions here and then see the tutorial on setting up a 
Jekyll website [here](https://help.github.com/articles/using-jekyll-as-a-static-site-generator-with-github-pages/).

### Step 1: Clone repository and edit settings

The first step is to click on the "Fork" button in this repository. It
will make a copy under your username that you can control.

Next, click on the Settings tab (in your fork). Rename the repository
**[USERNAME].github.io** with your exact username filled in.

Then, go back into the Setting page and scroll to the GitHub Pages
section. Select "master Branch" under Source. This will make your repostiory
show up as a website. It can sometimes take a while (upwards of 30 minutes or
more) for this first push to work, so that is why we are starting here.
There should be a link on the settings page once you do this pointing to your
new website.

### Step 2: Edit index.html

While waiting for the GitHub pages to set-up, go to the repository and edit
the file *index.html*. To do this, first click on the file and then click
on the pencil in the upper right-hand corner of the inner window (next to the
trashcan symbol). You will see three link on the page that all point to
the link **https://github.com/USERNAME**. Replace **USERNAME** with your 
GitHub username and then hit to green "Commit changes" button at the bottom
of the page. If you would like to redirect to a different website, you can
fill this site instead. Make sure that all three instances of the website
are the same.

### Step 3: Edit GitHub profile

If you are using your GitHub website as the location to redirect to, you
will want to fill out your user page. Go the [GitHub profile page](https://github.com/settings/profile)
and add a photo and some basic information. Make sure to save your changes.

### Step 4: Test

Finally, we can test that your page works by going to the website

- https://[USERNAME].github.io/

With your username filled in. If it says that no page is found, this may
be due to the GitHub servers not updating your website yet. Be patient as
it should appear with the first hour after your the first time you publish
it. If you get a "404 Page not found" this is likely due to not correctly
changing and saving the *index.html* file. Verify that you completed step
2 correctly. 

### Step 5: Share

You now have a digital identity that you can share with others that is
not tied to your place of study or employment. At any point you can 
redirect this link to your current location and contact information. If
you do not have a personal website to link to but want more than just
the basic information on your GitHub page, follow the Jekyll tutorial
[here](https://help.github.com/articles/using-jekyll-as-a-static-site-generator-with-github-pages/)
for setting up a complete website entirely hosted on GitHub. You can
see an example of this on my GitHub pages:

- [http://statsmaths.github.io/](http://statsmaths.github.io/)

Otherwise, share and enjoy!

--------------------------

# Tesseract Optical Character Recognition (OCR)
#### Description
Tesseract-OCR is an open source OCR (optical character recognition) engine, originally developed by Hewlett Packard Laboratories. The standard installation of Tesseract-OCR can convert images of text in 39 different languages to plain text data.  

#### Scenario
You visit an archive and need to capture images of text based archival collections for your research - ultimately you would like to convert these images into data that you can search, visualize, text mine, etc. Using a digital camera and/or a copier you capture photos of archival collections in the .tif / .tiff format. With these files in hand you are prepared to use Tesseract-OCR to convert your images into plain text files.

*Sometimes we get page images, but what we really need is plain text. Tesseract is free OCR software available in lots of languages that can generate text from images at a large scale.*

Navigate to the sevenagesofwoman folder (`$ cd Desktop/project/corpora/sevenagesofwoman`)

`$ cd sevenagesofwoman`

List files in the sevenagesofwoman folder

`$ ls`

Convert one tiff file to one txt file using Tesseract OCR

`$ tesseract sevenagesofwoman_thebride.tiff sevenagesofwoman_thebride`

Using your GUI, compare the tif file to the txt files you generated

While we’re here, why don’t we just OCR all of them in one batch?

`$ for i in *.tiff ; do tesseract $i $i; done;`
>*Remember our loop from the Command Line Bootcamp? This works the same way, but condenses everything to a single line using semicolons between commands.*

----
*FYI for Windows Users*

Find tesseract.exe (it’s probably in Program Files (x86)) and drag it
in. You can also try the path below and then hunt down the file if it
doesn’t work

`$ '/c/Program Files (x86)/Tesseract-OCR/tesseract.exe' sevenagesofwoman_thebride.tif sevenagesofwoman_thebride`

Using your GUI, compare the tif file to the txt files you generated

While we’re here, why don’t we just OCR all of them in one batch?

`$ for %i in (*.tif) do '/c/Program Files
(x86)/Tesseract-OCR/tesseract.exe' %i %i`

----

### EGREP and Regular Expressions

#### Searching & Mining

Move back to the BWRP books

`$ cd ..`

Find out how many lines and words there are in a text of your choosing using **wc -l -w**

`$ wc -l -w bwrp_ActoEPoems.txt`

Results:

```
1607 15242 bwrp_ActoEPoems.txt
```
>1607 lines and 15,242 words

Do some very basic searching with egrep — this will print the entire line it's mentioned in

`$ egrep europe *txt`

`$ egrep Europe *txt`

`$ egrep America *txt`

Do some very basic counting with **egrep -c**

`$ egrep -c man *txt`

`$ egrep -c woman *txt`

Count only whole words using **egrep -cw**

`$ egrep -cw man *txt`

The possibilities for regular expressions are endless (and sometimes difficult and always ugly), but you can also find matching patterns.

What 18th and 19th century years (or very similar four character numbers) are mentioned in these texts?
`$ egrep -o '\b1[7-8][0-9][0-9]\b' *txt`
> the -o flag returns just the text that matches the pattern
this looks for four numbers in a row that start with a 17 or 18—the rest can be any numbers

It's possible that some of these aren't years. They could be page numbers or amounts or anything else. Let's do a search that includes some context, but not the entire line.

`$ egrep -o '.{0,50}\b1[7-8][0-9][0-9]\b.{0,50}' *txt`

This context feature is interesting. What if we wanted to look at the words around 'America' to see what people are saying without getting every full line?

`$ egrep -o '.{0,50}America.{0,50}' *txt`

We could even move this into a separate corpus if we wanted by adding `> americacontext.text` to that search.
 

-----------------


# Text Analysis with Voyant 

Let's define Text Analysis. Are you familiar with this method? How would you define it?

For our corpus, we will explore the State of the Union addresses.  The State of the Union is delivered by the President of the United States annualy to a joint session of Congress. 
It is often a space where the President reflects on current issues and outlines goals for the nation. 
Therefore, it is a key document for understanding the ways the executive branch understands the current position 
of the country and their priorities. While today it is delivered oraly by the President, 
the State of the Union was initially a written document submitted to congress. 
In this lab, we will use Voyant to identify issues and priorities.


We will be using [Voyant](https://voyant-tools.org/):  a web-based text reading and analysis environment.

According to the Voyant Website <sup>[1](#myfootnote1)</sup>, we can do the folllowing:

- Use it to learn how computers-assisted analysis works. Check out our examples that show you how to do real academic tasks with Voyant.
- Use it to study texts that you find on the web or texts that you have carefully edited and have on your computer.
- Use it to add functionality to your online collections, journals, blogs or web sites so others can see through your texts with analytical tools.
- Use it to add interactive evidence to your essays that you publish online. Add interactive panels right into your research essays (if they can be published online) so your readers can recapitulate your results.
- Use it to develop your own tools using our functionality and code. 


### 2016 SOTU Speech
We will start with Obama's final State of the Union address.  

To begin, we will load in our text from [here](http://programminghistorian.github.io/ph-submissions/assets/basic-text-processing-in-r/sotu_text/236.txt).

Let's take a look at the speech. 
- What kind of file is this?  
- What does the format of this file tell us about one way that Voyant needs text to be structured to process it?

We can load data into Voyant three ways. 

1. Use the URL
2. Copy and paste the text into the box. 
3. Upload a file.


Now let's take a look at the kinds of text analysis used by Voyant!

#### Cirrus -  Terms - Links 

Cirrus:  Provides a word cloud of the most frequent terms. You can hover over the word to see the number of times it is used. 

- Are these the words we expected?
- Are there any words we would have expected that aren't included? 
- Are there any words we think should be removed?


#### Stop Words 

Stop Words are a list of common words that are filtered out before or during text processing. You can use default stop word lists, like those included in Voyant, or create your own.   

Let's say we want to remove "mdash" and add it to our stop words. Go to the bottom left panel that says "Summary Documents Phrases". Next to the question mark is another set of options that only appear once you hover over the area. The first option to the left of the question mark allows us to adjust our stop words. 

Voyant's default setting auto-detects a stop word list. Select "None" and see what happens! 
- Is this helpful?

Let's go back and select "English". To adjust our list, select "Edit List." Let's add "mdash". 

 - Are there any other we want to add? 

Let's add: "that's" and "it's". 

(Tip: When I'm adjusting the stop word list, I like to make a text file with my additional stop words. You'll notice Voyant only allows you to adjust your stop words once. If you try to add more, it deletes your previous custom words.)



#### Cirrus -  Terms - Links

Cirrus: Now we have a new word cloud!

Terms: We see the raw count of words in a list. 

Links: Provides a collocates graph shows a network graph of higher frequency terms that appear in proximity. Keywords are shown in green and collocates (words in proximity) are showing in red. 

Let's click on "America." Let's take a look at the Reader in the panel to the right. 
- What changed?

Now let's take a look at Trends in the panel furthest to the right. The default is Raw Frequencies. 

Let's change to Relative Frequenices. (This isn't as helpful with one document but will be when we are analyzing more than one at a time.)

Now let's go back to Links and double click on "America". 

- What changed?

#### Contexts - Bubblelines 

Contexts: Puts a term in context.

Bubblelines: A visualization of the term frequency in the document. 


#### Summary - Documents - Phrases
 
Summary: Overview of the document.  To increase the number of frequence words, adjust the items slider on the bottom left. 

Documents: We only have one document, but it will be helpful when we look at multiple at once.

Phrases: Provides a table of repreating phrases. 

Let's sort by the most common phrases. (Tip: If Voyant won't let you reset and see all the phraes, reload.)

Pair up and take a few minutes to explore. 
- Interesting insights? 


### Washington vs Obama

Let's now take a look at George Washington vs President Obama's SOTU addressess.


[Download the corpus](https://drive.google.com/open?id=0B6zkbDdW8bzIQnpQX2NQbVFYQjg) to your Downloads Folder. 

Unzip the file.

Go to [Voyant](https://voyant-tools.org/) and select "Upload".  
The speechs are named according to year. 
Make sure the files are in numerical order for this determines how Voyant loads them in.  Now, let's explore!

To begin, take a look at Cirrus. 
- Do we want to remove any stop words? If so, why?

Let's remove them. 

##### Summary - Documents - Phrases

- Can we learn anything from this? Document Length? Vocabulary Density? 

We also have a new option - Distinctive Words. 
Voyant uses Term Frequency-Inverse Document Frequency to weigh how important a word is in the document or corpus. 
Let's take a look at the terms used by Washington and Obama. 

- Are there any themes we can see in these speeches? By presidency? 

- Do we see particular phrases? 


##### Explore!

Interested in looking at all of the State of the Union addresses? Here you [go](https://programminghistorian.org/assets/basic-text-processing-in-r/sotu_text.zip)! 

Want to look at films plots from Wikipedia? Here you [go](https://github.com/dmics/voyant/blob/master/txt.zip)! 
- The "all" folder includes the plots from each film nominated for an Academy Award.
- The "win" folder inclues a subset, the plots from each film that won an Acadeym Award. 


If you are interested in how to work with the State of the Union addressed with the R programming language, see [my tutorial](https://programminghistorian.org/lessons/basic-text-processing-in-r) with Taylor Arnold on Programming Historian. 


PS: A quick note about lemmatization is necessary. Lemmatization may be important for your study. For example, if we are interested in how common the corpus talks talks about "states" then we need to search "state" and "states". By lemmatizing, all instances of "states" becomes "state". Voyant does a version of this. However, we can then take this a step further. What if I use the term once to mean the political boundaries  (ex. the state of Virginia) and as second time to mean a condition once was in (ex. in a state of happiness) We could then use Natural Language Processing (NLP) to distinguish between these two kinds.  Tools for lemmatization and NLP include [CleanNLP](https://cran.r-project.org/web/packages/cleanNLP/index.html) (for R) and [Lexos](https://cran.r-project.org/web/packages/cleanNLP/index.html) (command line). 

